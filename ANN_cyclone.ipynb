{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO/BHu9qqzXDzaCBSin0dd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritiksharmasde/Deep_Learning/blob/main/ANN_cyclone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wITTqhYQ5X0N",
        "outputId": "9245dc07-7f2e-4045-d4f5-758e853e71a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Sea_Surface_Temperature  Atmospheric_Pressure   Humidity  Wind_Shear  \\\n",
            "0                27.498160           1008.521429  89.279758   13.979877   \n",
            "1                28.404460           1001.242177  60.823380   19.548648   \n",
            "2                27.216969            995.742693  77.277801    9.368437   \n",
            "3                27.824280           1003.555279  67.986951   12.713517   \n",
            "4                26.260206           1008.466566  98.625281   17.125960   \n",
            "\n",
            "   Vorticity  Latitude  Ocean_Depth  Proximity_to_Coastline  \\\n",
            "0   0.000020  8.119890    76.137625                1.366176   \n",
            "1   0.000084  9.246782   131.821235                0.683405   \n",
            "2   0.000063  7.789877   181.465092                0.866362   \n",
            "3   0.000061  5.929008   323.395183                0.670524   \n",
            "4   0.000034  6.953442   357.904862                0.940152   \n",
            "\n",
            "   Pre_existing_Disturbance  Cyclone  \n",
            "0                         1        1  \n",
            "1                         1        1  \n",
            "2                         1        1  \n",
            "3                         1        1  \n",
            "4                         1        1  \n",
            "[('Sea_Surface_Temperature', 1.0), ('Atmospheric_Pressure', 1.0), ('Humidity', 1.0)]\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.9600 - loss: 0.3243 - val_accuracy: 1.0000 - val_loss: 0.0248\n",
            "Epoch 2/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9999 - loss: 0.0181 - val_accuracy: 1.0000 - val_loss: 0.0063\n",
            "Epoch 3/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 1.0000 - val_loss: 0.0028\n",
            "Epoch 4/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
            "Epoch 5/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 9.8476e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 7.7468e-04 - val_accuracy: 1.0000 - val_loss: 6.7461e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 5.0624e-04 - val_accuracy: 1.0000 - val_loss: 4.9073e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 4.6389e-04 - val_accuracy: 1.0000 - val_loss: 3.6662e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.9622e-04 - val_accuracy: 1.0000 - val_loss: 2.8408e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 2.3235e-04 - val_accuracy: 1.0000 - val_loss: 2.2472e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 2.3327e-04 - val_accuracy: 1.0000 - val_loss: 1.8166e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 1.7682e-04 - val_accuracy: 1.0000 - val_loss: 1.4950e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 1.5765e-04 - val_accuracy: 1.0000 - val_loss: 1.2429e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.1516e-04 - val_accuracy: 1.0000 - val_loss: 1.0414e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 8.9142e-05 - val_accuracy: 1.0000 - val_loss: 8.7833e-05\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 8.6062e-05  \n",
            "Test Accuracy: 1.0000\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
            "Predictions: [[0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]]\n",
            "[1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "file_path = \"cyclone_dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())\n",
        "\n",
        "# Function to calculate Entropy\n",
        "def entropy(s):\n",
        "    proportions = s.value_counts(normalize=True)\n",
        "    return -np.sum(proportions * np.log2(proportions))\n",
        "\n",
        "# Calculate Entropy for the entire dataset based on the target variable \"Cyclone\"\n",
        "entropy_total = entropy(df[\"Cyclone\"])\n",
        "\n",
        "# Calculate Information Gain for each feature\n",
        "information_gains = {}\n",
        "\n",
        "for feature in df.columns:\n",
        "    if feature != \"Cyclone\":\n",
        "        # Calculate the weighted entropy for this feature\n",
        "        weighted_entropy = 0\n",
        "        for value in df[feature].unique():\n",
        "            subset = df[df[feature] == value]\n",
        "            weighted_entropy += (len(subset) / len(df)) * entropy(subset[\"Cyclone\"])\n",
        "        # Information Gain = Entropy(Parent) - Weighted Entropy\n",
        "        information_gain = entropy_total - weighted_entropy\n",
        "        information_gains[feature] = information_gain\n",
        "\n",
        "# Get top 3 features with highest information gain\n",
        "top_3_features = sorted(information_gains.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "print(top_3_features)\n",
        "\n",
        "\n",
        "X = df.iloc[:, :-1].values  # Features\n",
        "y = df.iloc[:, -1].values   # Target\n",
        "\n",
        "# # Encode categorical target variable if necessary\n",
        "# if df.iloc[:, -1].dtype == 'object':  # If the target is categorical\n",
        "#     label_encoder = LabelEncoder()\n",
        "#     y = label_encoder.fit_transform(y)  # Convert categories to numerical values\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize/normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the ANN model\n",
        "model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
        "    Dense(16, activation='relu'),  # Hidden layer\n",
        "    Dense(1, activation='sigmoid')  # Output layer (use 'softmax' for multi-class classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=15, batch_size=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "predictions = (predictions > 0.5).astype(int)  # Convert probabilities to class labels\n",
        "print(\"Predictions:\", predictions[:10])  # Show first 10 predictions\n",
        "print(y[:10])"
      ]
    }
  ]
}